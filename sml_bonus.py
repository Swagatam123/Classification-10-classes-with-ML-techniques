# -*- coding: utf-8 -*-
"""SML_BONUS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IKyuIg-kRlhMqMTTgpvt_xtHGqhk0o69
"""

import cv2
import os
import random
from sklearn.tree import DecisionTreeClassifier
import numpy as np
import csv
import copy
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.ensemble import AdaBoostClassifier
from skimage.feature import local_binary_pattern
from skimage.feature import hog
from numpy.linalg import norm
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.decomposition import PCA
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
import csv
import skimage
from sklearn.model_selection import StratifiedKFold
from scipy.stats import itemfreq
from sklearn.preprocessing import normalize
from sklearn.svm import LinearSVC
from sklearn.neural_network import MLPClassifier
import pickle

!unzip cse342-542-bonus.zip

datapath="bonus_dataset/sml_train"
label_path="bonus_dataset/sml_train.csv"
test_datapath="bonus_dataset/sml_test"

#####################daisy feature extraction#################
def find_daisy(img):
    val=skimage.feature.daisy(img)
    bin = np.arange(257)
    hist = np.histogram(val.flatten(), bins=bin , density = True)
    return list(hist[0])

#####################laplacian ##########################
def find_laplace(img):
  laplacian = cv2.Laplacian(img,cv2.CV_64F)
  bin = np.arange(257)
  hist = np.histogram(laplacian.flatten(), bins=bin , density = True)
  return list(hist[0])

###############local binary pattern#######################
def find_lbp_1(img):
    radius = 1
    no_points = 8 * radius
    lbp = local_binary_pattern(img, no_points, radius, method='uniform')
    bin = np.arange(257)
    hist = np.histogram(lbp.flatten(), bins=bin , density = True)
    return list(hist[0])

###############local binary pattern#######################
def find_lbp(img):
    radius = 1
    no_points = 8 * radius
    #lbp = local_binary_pattern(img, no_points, radius, method='uniform')
    bin = np.arange(257)
    hist = np.histogram(img.flatten(), bins=bin , density = True)
    return list(hist[0])

#################################HOG TRANSFORM#################
def find_hog(img):
    #winSize = (32,32)
    #blockSize = (16,16)
    #blockStride = (8,8)
    #cellSize = (8,8)
    #nbins = 9
    #hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins)
    val=skimage.feature.hog(img, orientations=9, pixels_per_cell=(16, 16), cells_per_block=(3, 3))
    #val = hog.compute(img)
    return list(val.flatten())

###########################PCA############################
def pca_feature(X,test):
    pca=PCA(n_components=0.95)
    pca.fit(X)
    newX=pca.transform(X)
    new_test=pca.transform(test)
    return newX,new_test

def crossValidation(clff,data,label):
    k_fold = StratifiedKFold(n_splits=5)
    validation=[]
    for train_indices, test_indices in k_fold.split(data,label):
        temp=[train_indices,test_indices]
        validation.append(temp) 
    get_score=[]
    for i in range(5):
        data_train=[]
        label_train=[]
        data_test=[]
        label_test=[]
        for j in validation[i][0]:
            data_train.append(data[j])
            label_train.append(label[j])
        for j in validation[i][1]:
            data_test.append(data[j])
            label_test.append(label[j])
        clf=clff.fit(data_train,label_train)
        score=clf.score(data_test,label_test)
        print("Model " + str(i)+ " score: "+str(score))
        get_score.append(score)
    final_train_data=[]
    final_label_data=[]
    idx=np.argmax(get_score)
    for j in validation[idx][0]:
        final_train_data.append(data[j])
        final_label_data.append(label[j])
    return final_train_data,final_label_data

def feature_extraction(img):
    featureset=[]
    b,g,r=cv2.split(img)
    #featureset+=find_daisy(img)
    featureset+=find_lbp(b)
    featureset+=find_lbp(g)
    featureset+=find_lbp(r)
    #featureset+=image_moment_feature(img)
    featureset+=find_hog(r)
    featureset+=find_hog(g)
    featureset+=find_hog(b)
    #featureset+=find_daisy(b)
    #featureset+=find_daisy(g)
    #featureset+=find_daisy(r)
    featureset+=find_lbp_1(r)
    featureset+=find_lbp_1(g)
    featureset+=find_lbp_1(b)
    g_kernel = cv2.getGaborKernel((5, 5), 2.0, np.pi/16, 5.0, 0.5, 0, ktype=cv2.CV_32F)
    gimg = cv2.filter2D(img, cv2.CV_8UC3, g_kernel)
    b1,g1,r1=cv2.split(gimg)
    featureset+=find_lbp(b1)
    featureset+=find_lbp(g1)
    featureset+=find_lbp(r1)
    featureset+=find_hog(r1)
    featureset+=find_hog(g1)
    featureset+=find_hog(b1)
    featureset+=find_lbp_1(r1)
    featureset+=find_lbp_1(g1)
    featureset+=find_lbp_1(b1)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v=cv2.split(hsv)
    featureset+=find_lbp(h)
    featureset+=find_lbp(s)
    featureset+=find_lbp(v)
    featureset+=find_hog(h)
    featureset+=find_hog(s)
    featureset+=find_hog(v)
    featureset+=find_lbp_1(h)
    featureset+=find_lbp_1(s)
    featureset+=find_lbp_1(v)
    gimg = cv2.filter2D(hsv, cv2.CV_8UC3, g_kernel)
    h1,s1,v1=cv2.split(gimg)
    featureset+=find_lbp(h1)
    featureset+=find_lbp(s1)
    featureset+=find_lbp(v1)
    featureset+=find_hog(h1)
    featureset+=find_hog(s1)
    featureset+=find_hog(v1)
    featureset+=find_lbp_1(h1)
    featureset+=find_lbp_1(s1)
    featureset+=find_lbp_1(v1)
    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    #featureset+=find_laplace(gray)
    return featureset

def readdata(datapath,label_path):
    label_dataset=[]
    dataset=[]
    data=[]
    c=0
    with open(label_path, 'r') as csvfile:
        csvreader = csv.reader(csvfile)
        for row in csvreader:
            if row[1]=="Category":
                continue
            else:
                c+=1
                label_dataset.append(int(row[1]))
                file=row[0]
    #directory = os.listdir(datapath)
    #files_list = os.listdir(datapath)
    #for file in files_list:
                #img = cv2.imread(datapath+'/'+file)
                #dim=(32,32)
                #resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)
                gray= cv2.imread(datapath+'/'+file,1)
                #blur = cv2.blur(gray,(5,5))
                #kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
                #gray = cv2.filter2D(blur, -1, kernel)
                val=feature_extraction(gray)
                data.append(gray.flatten())
                #dataset.append(gray.flatten())
                dataset.append(val)
                #print(np.shape(dataset))
    return dataset,label_dataset,data

def readdata_test(datapath):
    test_dataset=[]
    label_name=[]
    directory = os.listdir(datapath)
    files_list = os.listdir(datapath)
    files_list.sort()
    for file in files_list:
                label_name.append(file)
                gray= cv2.imread(datapath+'/'+file,1)
                val=feature_extraction(gray)
                test_dataset.append(val)
    return test_dataset,label_name

dataset,label_dataset,data=readdata(datapath,label_path)

np.shape(dataset)

test_dataset,label_name=readdata_test(test_datapath)

##################dimensionality reduction###############
#dataset=copy.deepcopy(dataset)
dataset,test_dataset=pca_feature(dataset,test_dataset)

########################feature selection############################
dataset = SelectKBest(chi2, k=int(0.6*len(dataset[0]))).fit_transform(dataset, label_dataset)
print(np.shape(dataset))
#model = ExtraTreesClassifier()
#model.fit(dataset, label_dataset)
#imp=list(model.feature_importances_)
#for i in range(0,len(dataset)):
#    dataset[i] = [x for _,x in sorted(zip(imp,dataset[i]),reverse=True)]
#    dataset[i]=dataset[i][:1000]

np.shape(test_dataset)

#######################ISOLATION FOREST outliers detection############################
rng = np.random.RandomState(42)
clf = IsolationForest(behaviour='new', max_samples=200,random_state=rng, contamination='auto')
clf.fit(dataset)
outliers = clf.predict(dataset)

##########################LOCAL OUTLIER FACTOR#######################################
clf = IsolationForest()
outliers = clf.fit_predict(dataset)

final_data=[]
final_label=[]
for i in range(0,len(list(outliers))):
    if outliers[i]==1:
        final_data.append(dataset[i])
        final_label.append(label_dataset[i])
#dataset=copy.deepcopy(final_data)
#label_dataset=copy.deepcopy(final_label)

########################test feature selection############################
test_dataset = SelectKBest(chi2, k=int(0.6*len(test_dataset[0]))).fit(test_dataset)
print(np.shape(test_dataset))

##################################PREDCITION#################################
#train_data,test_data,train_label,test_label=train_test_split(dataset, label_dataset, test_size=0.3, random_state=42)
#classify=DecisionTreeClassifier()
#clf=classify.fit(train_data,train_label)
#predict_label=clf.predict(test_data)
#score=accuracy_score(test_label, predict_label)
#print(score)
#clf = GaussianNB()
#train_data,train_label=crossValidation(clf,final_data,final_label)
#clf.fit(dataset, label_dataset)
clf = LinearSVC()
#train_data,train_label=crossValidation(clf,final_data,final_label)
clf.fit(final_data, final_label) 
predict_label=clf.predict(test_dataset)
#print(np.shape(predict_label))
#for i in range(0,len(predict_label)):
#               print(str(label_name[i])+" , "+str(predict_label[i]))
#score=accuracy_score(test_label, predict_label)
#print(score)

def subsample(dataset,labelset,sample_size):
  index=np.random.randint(len(dataset),size=sample_size)
  data_sample=[]
  label_sample=[]
  for ind in index:
      data_sample.append(dataset[ind])
      label_sample.append(labelset[ind])
  return data_sample,label_sample

#########################SVC CLASSIFIER##########################
pr=np.zeros((1000,5))
models=[]
for j in range(0,5):
    #data_sample,label_sample=subsample(final_data,final_label,len(final_data))
    clf = LinearSVC()
    clf.fit(final_data, final_label)
    models.append(clf)
    predict_label=clf.predict(test_dataset)
    for i in range(0,1000):
      pr[i][j]=predict_label[i]
    print(j)
#predict_label=[]
#for i in range(0,1000):
#  counts = np.bincount(pr[i])
#  predict_label.append(np.argmax(counts))

#index=random.sample(range(10), 5)
#print(index)
#pr_select=np.zeros((1000,5))
#for i in range(0,1000):
#  for j in range(0,5):
#      pr_select[i][j]=pr[i][index[j]]
#print(np.shape(pr_select))
predict_label=[]
for i in range(0,1000):
  (values,counts) = np.unique(pr[i],return_counts=True)
  predict_label.append(int(values[np.argmax(counts)]))

pr=np.zeros((1000,3))
clf=MLPClassifier()
clf.fit(final_data, final_label) 
models.append(clf)
predict_label3=clf.predict(test_dataset)
for i in range(0,1000):
    pr[i][0]=predict_label[i]
    pr[i][1]=predict_label3[i]
#clf=GaussianNB()
#clf.fit(final_data, final_label) 
#predict_label4=clf.predict(test_dataset)
#for i in range(0,1000):
#    pr[i][2]=predict_label4[i]

##########################GAUSSIAN CLASSIFIER####################
pr2=np.zeros((1000,20))
for j in range(0,20):
    data_sample,label_sample=subsample(final_data,final_label,len(final_data))
    clf = GaussianNB()
    clf.fit(data_sample, label_sample) 
    models.append(clf)
    predict_label2=clf.predict(test_dataset)
    for i in range(0,1000):
      pr2[i][j]=predict_label2[i]
    print(j)

predict_label2=[]
for i in range(0,1000):
  (values,counts) = np.unique(pr2[i],return_counts=True)
  predict_label2.append(int(values[np.argmax(counts)]))
for i in range(0,1000):
  pr[i][2]=predict_label2[i]

predict_label_f1=[]
for i in range(0,1000):
  (values,counts) = np.unique(pr[i],return_counts=True)
  predict_label_f1.append(int(values[np.argmax(counts)]))

import csv
head=["Id","Category"]
with open('MT18146_SWAGATAM_submission.csv', 'w') as writeFile:
    writer = csv.writer(writeFile)
    writer.writerow(head)
    for i in range(0,len(predict_label_f1)):
      row=[]
      row.append(label_name[i])
      row.append(predict_label_f1[i])
      writer.writerow(row)

writeFile.close()

pickle_out = open("models.pickle","wb")
pickle.dump(models, pickle_out)
pickle_out.close()